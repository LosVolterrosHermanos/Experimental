import jax.numpy as jnp
import jax.random as random
import scipy as sp

class power_law_RF:
  """
  A class that generates power-law random features regression problems.

  This class creates synthetic regression problems with power-law decaying eigenvalues 
  and target coefficients. The features are generated by first sampling random Gaussian 
  features and then scaling them according to a power law.

  Attributes:
      alpha (float): Power law exponent for eigenvalue decay
      beta (float): Power law exponent for target coefficient decay  
      W (ndarray): Random features matrix of shape (v, d)
      v (int): Hidden dimensionality
      d (int): Embedded dimensionality
      x_grid (ndarray): Grid of indices from 1 to v, shape (1,v)
      population_eigenvalues (ndarray): Power-law decaying eigenvalues
      b (ndarray): Power-law decaying target coefficients
      population_trace (float): Sum of population eigenvalues
      checkW (ndarray): Scaled random features matrix
      checkb (ndarray): Scaled target coefficients
  """

  def __init__(self, alpha, beta, W):
      self.alpha = alpha
      self.beta = beta
      self.W = W
      self.v = self.W.shape[0]
      self.d = self.W.shape[1]
      self.x_grid=jnp.arange(1, self.v+1).reshape(1,self.v)
      self.population_eigenvalues = self.x_grid**(-self.alpha)
      self.b = self.x_grid.transpose()**(-beta)
      self.population_trace = jnp.sum(self.population_eigenvalues)
      self.checkW = W * self.population_eigenvalues.T
      self.checkb = self.x_grid.transpose()**(-alpha-beta)
      
  @classmethod
  def initialize_random(cls, alpha, beta, v, d, key):
      """
      Creates a new power_law_RF instance with randomly initialized features.

      Args:
          alpha (float): Power law exponent for eigenvalue decay
          beta (float): Power law exponent for target coefficient decay
          v (int): Hidden dimensionality
          d (int): Embedded dimensionality
          key (PRNGKey): JAX random number generator key

      Returns:
          power_law_RF: A new instance with randomly sampled features matrix W
                        scaled to have variance 1/d
      """
      # Sample random features matrix with variance 1/d
      W = random.normal(key, (v, d)) / jnp.sqrt(d)
      return cls(alpha=alpha, beta=beta, W=W)
  
  def get_population_risk(self, w):
      """
      Calculates the population risk for given weights.
      
      The population risk is the expected squared error over the data distribution.
      For power-law random features regression, this can be computed analytically
      without sampling data.
      
      Args:
          w (ndarray): Weight vector of shape (d,)
      
      Returns:
          float: Population risk value
      """
      # Project weights onto random features
      proj = jnp.matmul(self.checkW, w)
      
      # Calculate population risk using eigenvalues and target coefficients
      risk = jnp.sum((proj - self.checkb)**2)
      return risk / 2
  

  def get_data(self, key, batch):
      """
      Generates a batch of synthetic data points.
      
      Args:
          key (PRNGKey): JAX random number generator key
          batch (int): Number of data points to generate
          
      Returns:
          tuple: (X, y) where:
              X (ndarray): Input features of shape (batch, d)
              y (ndarray): Target values of shape (batch, 1)
      """
      # Generate random features
      x = random.normal(key, (batch, self.v))
      
      return jnp.matmul(x, self.checkW), jnp.matmul(x, self.checkb)
  
  def get_theory_limitloss(self):
      """Returns the theoretical limit of the loss (residual risk) for the current model parameters.
      
      Calculates the theoretical prediction for the residual risk level (risk at infinite time)
      using the model's alpha, beta, v (number of random features), and d (input dimension) parameters.
      
      Returns:
          float: Theoretical prediction for the residual risk level
      """
      return power_law_RF.theory_limitloss(self.alpha,self.beta,self.v,self.d)
  
  def theory_limitloss(alpha, beta,V,D):
      """Generate the 'exact' finite V, D expression the residual risk level (risk at time infinity)

      Parameters
      ----------
      alpha,beta : floats
          parameters of the model, ASSUMES V>D

      Returns
      -------
      theoretical prediction for the norm
      """
      cstar = 0.0
      if 2*alpha >= 1.0:
          kappa = power_law_RF.theory_kappa(alpha,V,D)
          cstar = jnp.sum( jnp.arange(1,V,1.0)**(-2.0*(beta+alpha))/( jnp.arange(1,V,1.0)**(-2.0*(alpha))*kappa*(D**(2*alpha)) + 1.0))

      if 2*alpha < 1.0:
          #tau = D/jnp.sum( jnp.arange(1,V,1.0)**(-2.0*alpha))
          tau = power_law_RF.theory_tau(alpha,V,D)
          cstar = jnp.sum( jnp.arange(1,V,1.0)**(-2.0*(beta+alpha))/( jnp.arange(1,V,1.0)**(-2.0*(alpha))*tau + 1.0))


      return cstar
  
  def theory_kappa(alpha, V,D):
      """Generate coefficient kappa with finite sample corrections.
      Parameters
      ----------
      alpha : float
          parameter of the model.
      V,D : integers
          parameters of the model.

      Returns
      -------
      theoretical prediction for kappa parameter
      """

      TMAX = 1000.0
      c, _ = sp.integrate.quad(lambda x: 1.0/(1.0+x**(2*alpha)),0.0,TMAX)
      kappa=c**(-2.0*alpha)

      kappa_it = lambda k : sp.integrate.quad(lambda x: 1.0/(k+x**(2*alpha)),0.0,V/D)[0]
      eps = 10E-4
      error = 1.0
      while error > eps:
          kappa1 = 1.0/kappa_it(kappa)
          error = abs(kappa1/kappa - 1.0)
          kappa = kappa1
      return kappa
  
  def theory_tau(alpha, V,D):
      """Generate coefficient tau with finite sample corrections.
      Parameters
      ----------
      alpha : float
          parameter of the model.
      V,D : integers
          parameters of the model.

      Returns
      -------
      theoretical prediction for kappa parameter
      """

      tau_it = lambda k : jnp.sum( 1.0/(D*(jnp.arange(1,V,1)**(2*alpha) +k)))
      tau = tau_it(0)
      eps = 10E-4
      error = 1.0
      while error > eps:
          tau1 = 1.0/tau_it(tau)
          error = abs(tau1/tau - 1.0)
          tau = tau1
      return tau
  
  def get_hessian_spectra(self):
      """Get eigenvalues of the Hessian matrix of the problem

      Returns
      -------
      ndarray
          Array containing the eigenvalues of the Hessian matrix, computed as 
          the squared singular values of the checkW matrix.
      """
      _, s, _ =jnp.linalg.svd(self.checkW,full_matrices=False)
      return s**2

  def get_rhos(self):
      """Get squared-projections of the residual (b) in the direction of the eigenmodes of the Hessian.

      Returns
      -------
      ndarray
          Array containing the squared-projections of the residual vector b onto the eigenvectors
          of the Hessian matrix, normalized by the corresponding eigenvalues.
      """
      Uvec, s, _ =jnp.linalg.svd(self.checkW,full_matrices=False)

      #Compute < ( D^1/2 W W^T D^(1/2) - z)^{-1}, D^(1/2) b >
      check_beta_weight = jnp.tensordot(self.checkb,Uvec,axes=[[0],[0]])[0]

      rhos = (check_beta_weight)**2 / s**2
      rhos.astype(jnp.float32)
      return rhos




def ode_resolvent_log_implicit_full(eigs_K, rho_init, chi_init, sigma_init, risk_infinity,
                  g1, g2, g3, delta, batch, D, t_max, Dt):
  """Generate the theoretical solution to momentum

  Parameters
  ----------
  eigs_K : array d
      eigenvalues of covariance matrix (W^TDW)
  rho_init : array d
    initial rho_j's (rho_j^2)
  chi_init : array (d)
      initialization of chi's
  sigma_init : array (d)
      initialization of sigma's (xi^2_j)
  risk_infinity : scalar
      represents the risk value at time infinity
  WtranD : array (v x d)
      WtranD where D = diag(j^(-2alpha)) and W is the random matrix
  alpha : float
      data complexity
  V : float
      vocabulary size
  g1, g2, g3 : function(time)
      learning rate functions
  delta : function(time)
      momentum function
  batch : int
      batch size
  D : int
      number of eigenvalues (i.e. shape of eigs_K)
  t_max : float
      The number of epochs
  Dt : float
      time step used in Euler

  Returns
  -------
  t_grid: numpy.array(float)
      the time steps used, which will discretize (0,t_max) into n_grid points
  risks: numpy.array(float)
      the values of the risk

  """
  #times = jnp.arange(0, t_max, step = Dt, dtype= jnp.float64)
  times = jnp.arange(0, jnp.log(t_max), step = Dt, dtype= jnp.float32)

  risk_init = risk_infinity + jnp.sum(eigs_K * rho_init)

  def inverse_3x3(Omega):
      # Extract matrix elements
      a11, a12, a13 = Omega[0][0], Omega[0][1], Omega[0][2]
      a21, a22, a23 = Omega[1][0], Omega[1][1], Omega[1][2]
      a31, a32, a33 = Omega[2][0], Omega[2][1], Omega[2][2]

      # Calculate determinant
      det = (a11*a22*a33 + a12*a23*a31 + a13*a21*a32
            - a13*a22*a31 - a11*a23*a32 - a12*a21*a33)

      #if abs(det) < 1e-10:
      #    raise ValueError("Matrix is singular or nearly singular")

      # Calculate each element of inverse matrix
      inv = [[0,0,0],[0,0,0],[0,0,0]]

      inv[0][0] = (a22*a33 - a23*a32) / det
      inv[0][1] = (a13*a32 - a12*a33) / det
      inv[0][2] = (a12*a23 - a13*a22) / det

      inv[1][0] = (a23*a31 - a21*a33) / det
      inv[1][1] = (a11*a33 - a13*a31) / det
      inv[1][2] = (a13*a21 - a11*a23) / det

      inv[2][0] = (a21*a32 - a22*a31) / det
      inv[2][1] = (a12*a31 - a11*a32) / det
      inv[2][2] = (a11*a22 - a12*a21) / det

      return jnp.array(inv)

  def odeUpdate(stuff, time):
    v, risk = stuff
    timePlus = jnp.exp(time + Dt)

    Omega11 = -2.0 * batch * g2(timePlus) * eigs_K + batch * (batch + 1.0) * g2(timePlus)**2 * eigs_K**2
    Omega12 = g3(timePlus)**2 * jnp.ones_like(eigs_K)
    Omega13 = 2.0 * g3(timePlus) * (-1.0 + g2(timePlus) * batch * eigs_K)
    Omega1 = jnp.array([Omega11, Omega12, Omega13])

    Omega21 = batch * (batch + 1.0) * g1(timePlus)**2 * eigs_K**2
    Omega22 = ( -2.0 * delta(timePlus) + delta(timePlus)**2 ) * jnp.ones_like(eigs_K)
    Omega23 = 2.0 * g1(timePlus) * eigs_K * batch * ( 1.0 - delta(timePlus) )
    Omega2 = jnp.array([Omega21, Omega22, Omega23])

    Omega31 = g1(timePlus) * batch * eigs_K
    Omega32 = -g3(timePlus) * jnp.ones_like(eigs_K)
    Omega33 = -delta(timePlus) - g2(timePlus) * batch * eigs_K
    Omega3 = jnp.array([Omega31, Omega32, Omega33])

    Omega = jnp.array([Omega1, Omega2, Omega3]) #3 x 3 x d

    Identity = jnp.tensordot( jnp.eye(3), jnp.ones(D), 0 )

    A = inverse_3x3(Identity - (Dt * timePlus) * Omega) #3 x 3 x d

    Gamma = jnp.array([batch * g2(timePlus)**2, batch * g1(timePlus)**2, 0.0])
    z = jnp.einsum('i, j -> ij', jnp.array([1.0, 0.0, 0.0]), eigs_K)
    G_Lambda = jnp.einsum('i,j->ij', Gamma, eigs_K) #3 x d

    x_temp = v + Dt * timePlus * risk_infinity * G_Lambda
    x = jnp.einsum('ijk, jk -> ik', A, x_temp)

    y = jnp.einsum('ijk, jk -> ik', A, G_Lambda)

    vNew = x + ( Dt * timePlus * y * jnp.sum(x * z) / (1.0 - Dt * timePlus * jnp.sum(y * z)) )
    #vNew = vNew.at[0].set(jnp.maximum(vNew[0], 0.0))
    #vNew[0] = jnp.maximum(vNew[0], 10**(-7))
    #vNew[2] = jnp.maximum(vNew[2], 10**(-7))

    riskNew = risk_infinity + jnp.sum(eigs_K * vNew[0])
    return (vNew, riskNew), risk #(risk, vNew[0])

  _, risks = jax.lax.scan(odeUpdate,(jnp.array([rho_init,sigma_init, chi_init]),risk_init),times)
  return jnp.exp(times), risks/2