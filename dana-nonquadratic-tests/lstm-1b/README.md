This is an experiment to get DANA working on an lstm training task on the million-word dataset.

This is a reimplementation of the work of https://github.com/rafaljozefowicz/lm 

It uses the 1b_word_vocab.txt file from that repository https://github.com/rafaljozefowicz/lm/1b_word_vocab.txt.

It also uses the 1 Billion Word Language Model Benchmark: https://www.statmt.org/lm-benchmark/  
See also the paper of Chelba et al. : https://arxiv.org/abs/1312.3005

Plotting code is in dana_powerlaw_fits.py and plot_all_dana_curves.py (for comparison curves across various alpha)


