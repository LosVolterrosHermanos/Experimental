{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9C9v01qU6e5u"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "seed = 10\n",
        "key = jax.random.PRNGKey(seed)\n",
        "\n",
        "#def enable_float64():\n",
        "#  \"\"\"Tell jax to enable float64.\"\"\"\n",
        "#  jax.config.update('jax_enable_x64', True)\n",
        "\n",
        "#enable_float64()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD"
      ],
      "metadata": {
        "id": "22fhRhQa6qPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jax_lsq_momentum1(key,\n",
        "                g1, g2, g3, delta, batch, steps, init_x, init_w,\n",
        "                t_oracle, loss, loss_times = jnp.array([0])\n",
        "                ):\n",
        "  \"\"\" This routine generates losses for SGD on the least squares\n",
        "  problem with scalar targets, constant learning rate and constant batch size.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  key : PRNGKey\n",
        "    Jax PRNGKey\n",
        "  g1, g2, g3 : function(time)\n",
        "    The learning rate functions\n",
        "  delta : function(time)\n",
        "    The momentum function\n",
        "  batch : int\n",
        "    The batch-size to use\n",
        "  steps : int\n",
        "    The number of steps of minibatch SGD to generate\n",
        "  init_x : vector\n",
        "    The initial state for SGD to use\n",
        "  init_w : vector\n",
        "    The initial state for momentum\n",
        "  traceK : float\n",
        "    Trace of the covariance matrix of the data; i.e., K = E[aa^T]\n",
        "  t_oracle: callable\n",
        "    Takes as an argument a jax RNG key and a batch-size.\n",
        "    Expects in return two tensors (A, y)\n",
        "    of dimension (batch x data-dimension) and dimension (batch).\n",
        "  loss: callable\n",
        "    Takes as an argument a vector of length data-dimension,\n",
        "    which is the current linear model parameters, and returns the\n",
        "    loss.\n",
        "  loss_times: vector\n",
        "    Iteration counts at which to compute the loss\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  losses: vector\n",
        "    An array of length 'steps' containing the losses\n",
        "  loss_times: vector\n",
        "    Iteration counts at which the losses were computed\n",
        "  \"\"\"\n",
        "\n",
        "  if loss_times.shape[0]==1:\n",
        "    loss_times = jnp.arange(steps)\n",
        "  x = jnp.reshape(init_x,(len(init_x),1))\n",
        "  w = jnp.reshape(init_w,(len(init_w),1))\n",
        "\n",
        "\n",
        "  def update(z, things):\n",
        "    keyz, iteration = things\n",
        "    A,y = t_oracle(keyz, batch)\n",
        "    x,w = z\n",
        "    grad = jnp.tensordot(A,jnp.tensordot(A,x,axes=1)-y,axes=[[0],[0]])\n",
        "    neww = (1.0 - delta(iteration)) * w + g1(iteration) * grad\n",
        "    newx = x - g2(iteration) * grad - neww * g3(iteration)\n",
        "    return (newx,neww), x\n",
        "\n",
        "  keys=jax.random.split(key,steps)\n",
        "  iters =  jnp.linspace(0.0, steps, num = steps)\n",
        "\n",
        "\n",
        " # update_jit = jax.jit(update)\n",
        "  _, states = jax.lax.scan(update,(x,w),(keys,iters))\n",
        "\n",
        "  return jax.lax.map(loss, states[loss_times[loss_times< steps]]), loss_times[loss_times< steps], states[-1]"
      ],
      "metadata": {
        "id": "dW5v1cJQ6n38"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jax_lsq_momentum1_opt2(key,\n",
        "                g1, g2, g3, delta, batch, steps, init_x, init_w,\n",
        "                t_oracle, loss\n",
        "                ):\n",
        "  \"\"\" This routine generates losses for SGD on the least squares\n",
        "  problem with scalar targets, constant learning rate and constant batch size.\n",
        "  It has reasonable memory efficiency than the optimized version by only storing losses at loss_times.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  key : PRNGKey\n",
        "    Jax PRNGKey\n",
        "  lr1 : float\n",
        "    The learning rate to use \\gamma_1; should be constant/(tr(K)^2)\n",
        "  lr2 : float\n",
        "    The learning rate to use \\gamma_2; should be constant/(tr(K))\n",
        "  theta : float\n",
        "    The momentum parameter to use\n",
        "  batch : int\n",
        "    The batch-size to use\n",
        "  steps : int\n",
        "    The number of steps of minibatch SGD to generate\n",
        "  init_x : vector\n",
        "    The initial state for SGD to use\n",
        "  init_w : vector\n",
        "    The initial state for momentum\n",
        "  traceK : float\n",
        "    Trace of the covariance matrix of the data; i.e., K = E[aa^T]\n",
        "  t_oracle: callable\n",
        "    Takes as an argument a jax RNG key and a batch-size.\n",
        "    Expects in return two tensors (A, y)\n",
        "    of dimension (batch x data-dimension) and dimension (batch).\n",
        "  loss: callable\n",
        "    Takes as an argument a vector of length data-dimension,\n",
        "    which is the current linear model parameters, and returns the\n",
        "    loss.\n",
        "  loss_times: vector\n",
        "    Iteration counts at which to compute the loss\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  losses: vector\n",
        "    An array of length 'steps' containing the losses\n",
        "  loss_times: vector\n",
        "    Iteration counts at which the losses were computed\n",
        "  \"\"\"\n",
        "\n",
        "  if steps < 10**5:\n",
        "    return jax_lsq_momentum1(key,\n",
        "                g1, g2, g3, delta, batch, steps, init_x, init_w,\n",
        "                t_oracle, loss)\n",
        "  x = jnp.reshape(init_x,(len(init_x),1))\n",
        "  w = jnp.reshape(init_w,(len(init_w),1))\n",
        "\n",
        "  def update(z, things): #things = keys for your stochastic updates and momentum terms\n",
        "    keyz, iteration = things\n",
        "    A,y = t_oracle(keyz, batch)\n",
        "    x,w = z\n",
        "    #delta = theta / (iteration + traceK)\n",
        "    grad = jnp.tensordot(A,jnp.tensordot(A,x,axes=1)-y,axes=[[0],[0]])\n",
        "    neww = (1.0 - delta(iteration)) * w + g1(iteration) * grad\n",
        "    newx = x - g2(iteration) * grad - neww * g3(iteration)\n",
        "    return (newx,neww), x\n",
        "\n",
        "  def skinny_update(z, things):\n",
        "    keyz, iteration = things\n",
        "    A,y = t_oracle(keyz, batch)\n",
        "    x,w = z\n",
        "    #delta = theta / (iteration + traceK)\n",
        "    grad = jnp.tensordot(A,jnp.tensordot(A,x,axes=1)-y,axes=[[0],[0]])\n",
        "    neww = ( 1.0 - delta(iteration) ) * w + g1(iteration) * grad\n",
        "    newx = x - g2(iteration) * grad - neww * g3(iteration)\n",
        "    return (newx,neww), False\n",
        "\n",
        "  p = np.int32(np.ceil(np.log10(steps+1)))\n",
        "\n",
        "  mkey1,mkey2= jax.random.split(key)\n",
        "  keys=jax.random.split(mkey1,10**5)\n",
        "  #deltas = theta / ( jnp.linspace(0.0, 10**5, num = 10**5) + traceK)\n",
        "  iters = jnp.linspace(0.0, 10**5, num = 10**5)\n",
        "\n",
        " # update_jit = jax.jit(update)\n",
        "  z, states = jax.lax.scan(update,(x,w),(keys, iters))\n",
        "\n",
        "  losslist = jax.lax.map(loss,states)\n",
        "  timelist = jnp.arange(1,10**5+1,1)\n",
        "  lastiter = 10**5\n",
        "\n",
        "  mkeyout =  jax.random.split(mkey2,p-5)\n",
        "  for j, mkey in enumerate(mkeyout,start=5):\n",
        "    u=j-2\n",
        "    def outerloop(xw, thingz):\n",
        "      keyz,currentiter = thingz\n",
        "      mkeys = jax.random.split(keyz, 10**u)\n",
        "      iterlist = currentiter + jnp.arange(0, 10**u, dtype = jnp.float32)\n",
        "      #deltas= theta / (iterlist + traceK)\n",
        "      (newx,neww), _ = jax.lax.scan(skinny_update,xw,(mkeys,iterlist))\n",
        "      return (newx,neww), loss(newx)\n",
        "    outerloopsteps = min( (steps-lastiter)//(10**u), 100)\n",
        "    #outerloopitercounts = lastiter + (10**u)*jnp.arange(1,outerloopsteps+1,1)\n",
        "    outerloopitercounts = lastiter + (10**u)*jnp.arange(0,outerloopsteps,1)\n",
        "    keys=jax.random.split(mkey,outerloopsteps)\n",
        "    z, late_loss = jax.lax.scan(outerloop,z,(keys,outerloopitercounts))\n",
        "    losslist=jnp.concatenate([losslist,late_loss])\n",
        "    timelist =jnp.concatenate([timelist,outerloopitercounts])\n",
        "    lastiter += 10**j\n",
        "  return losslist, timelist, z[0]\n",
        "\n",
        "  # return losses,loss_times"
      ],
      "metadata": {
        "id": "VPqKdpgD6vr8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ODE"
      ],
      "metadata": {
        "id": "9szNEl2O7HRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ode_resolvent_log_implicit_full(eigs_K, rho_init, chi_init, sigma_init, risk_infinity,\n",
        "                  g1, g2, g3, delta, batch, D, t_max, Dt):\n",
        "  \"\"\"Generate the theoretical solution to momentum\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  eigs_K : array d\n",
        "      eigenvalues of covariance matrix (W^TDW)\n",
        "  rho_init : array d\n",
        "    initial rho_j's (rho_j^2)\n",
        "  chi_init : array (d)\n",
        "      initialization of chi's\n",
        "  sigma_init : array (d)\n",
        "      initialization of sigma's (xi^2_j)\n",
        "  risk_infinity : scalar\n",
        "      represents the risk value at time infinity\n",
        "  WtranD : array (v x d)\n",
        "      WtranD where D = diag(j^(-2alpha)) and W is the random matrix\n",
        "  alpha : float\n",
        "      data complexity\n",
        "  V : float\n",
        "      vocabulary size\n",
        "  g1, g2, g3 : function(time)\n",
        "      learning rate functions\n",
        "  delta : function(time)\n",
        "      momentum function\n",
        "  batch : int\n",
        "      batch size\n",
        "  D : int\n",
        "      number of eigenvalues (i.e. shape of eigs_K)\n",
        "  t_max : float\n",
        "      The number of epochs\n",
        "  Dt : float\n",
        "      time step used in Euler\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  t_grid: numpy.array(float)\n",
        "      the time steps used, which will discretize (0,t_max) into n_grid points\n",
        "  risks: numpy.array(float)\n",
        "      the values of the risk\n",
        "\n",
        "  \"\"\"\n",
        "  #times = jnp.arange(0, t_max, step = Dt, dtype= jnp.float64)\n",
        "  times = jnp.arange(0, jnp.log(t_max), step = Dt, dtype= jnp.float32)\n",
        "\n",
        "  risk_init = risk_infinity + jnp.sum(eigs_K * rho_init)\n",
        "\n",
        "  def inverse_3x3(Omega):\n",
        "      # Extract matrix elements\n",
        "      a11, a12, a13 = Omega[0][0], Omega[0][1], Omega[0][2]\n",
        "      a21, a22, a23 = Omega[1][0], Omega[1][1], Omega[1][2]\n",
        "      a31, a32, a33 = Omega[2][0], Omega[2][1], Omega[2][2]\n",
        "\n",
        "      # Calculate determinant\n",
        "      det = (a11*a22*a33 + a12*a23*a31 + a13*a21*a32\n",
        "            - a13*a22*a31 - a11*a23*a32 - a12*a21*a33)\n",
        "\n",
        "      #if abs(det) < 1e-10:\n",
        "      #    raise ValueError(\"Matrix is singular or nearly singular\")\n",
        "\n",
        "      # Calculate each element of inverse matrix\n",
        "      inv = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "\n",
        "      inv[0][0] = (a22*a33 - a23*a32) / det\n",
        "      inv[0][1] = (a13*a32 - a12*a33) / det\n",
        "      inv[0][2] = (a12*a23 - a13*a22) / det\n",
        "\n",
        "      inv[1][0] = (a23*a31 - a21*a33) / det\n",
        "      inv[1][1] = (a11*a33 - a13*a31) / det\n",
        "      inv[1][2] = (a13*a21 - a11*a23) / det\n",
        "\n",
        "      inv[2][0] = (a21*a32 - a22*a31) / det\n",
        "      inv[2][1] = (a12*a31 - a11*a32) / det\n",
        "      inv[2][2] = (a11*a22 - a12*a21) / det\n",
        "\n",
        "      return jnp.array(inv)\n",
        "\n",
        "  def odeUpdate(stuff, time):\n",
        "    v, risk = stuff\n",
        "    timePlus = jnp.exp(time + Dt)\n",
        "\n",
        "    Omega11 = -2.0 * batch * g2(timePlus) * eigs_K + batch * (batch + 1.0) * g2(timePlus)**2 * eigs_K**2\n",
        "    Omega12 = g3(timePlus)**2 * jnp.ones_like(eigs_K)\n",
        "    Omega13 = 2.0 * g3(timePlus) * (-1.0 + g2(timePlus) * batch * eigs_K)\n",
        "    Omega1 = jnp.array([Omega11, Omega12, Omega13])\n",
        "\n",
        "    Omega21 = batch * (batch + 1.0) * g1(timePlus)**2 * eigs_K**2\n",
        "    Omega22 = ( -2.0 * delta(timePlus) + delta(timePlus)**2 ) * jnp.ones_like(eigs_K)\n",
        "    Omega23 = 2.0 * g1(timePlus) * eigs_K * batch * ( 1.0 - delta(timePlus) )\n",
        "    Omega2 = jnp.array([Omega21, Omega22, Omega23])\n",
        "\n",
        "    Omega31 = g1(timePlus) * batch * eigs_K\n",
        "    Omega32 = -g3(timePlus) * jnp.ones_like(eigs_K)\n",
        "    Omega33 = -delta(timePlus) - g2(timePlus) * batch * eigs_K\n",
        "    Omega3 = jnp.array([Omega31, Omega32, Omega33])\n",
        "\n",
        "    Omega = jnp.array([Omega1, Omega2, Omega3]) #3 x 3 x d\n",
        "\n",
        "    Identity = jnp.tensordot( jnp.eye(3), jnp.ones(D), 0 )\n",
        "\n",
        "    A = inverse_3x3(Identity - (Dt * timePlus) * Omega) #3 x 3 x d\n",
        "\n",
        "    Gamma = jnp.array([batch * g2(timePlus)**2, batch * g1(timePlus)**2, 0.0])\n",
        "    z = jnp.einsum('i, j -> ij', jnp.array([1.0, 0.0, 0.0]), eigs_K)\n",
        "    G_Lambda = jnp.einsum('i,j->ij', Gamma, eigs_K) #3 x d\n",
        "\n",
        "    x_temp = v + Dt * timePlus * risk_infinity * G_Lambda\n",
        "    x = jnp.einsum('ijk, jk -> ik', A, x_temp)\n",
        "\n",
        "    y = jnp.einsum('ijk, jk -> ik', A, G_Lambda)\n",
        "\n",
        "    vNew = x + ( Dt * timePlus * y * jnp.sum(x * z) / (1.0 - Dt * timePlus * jnp.sum(y * z)) )\n",
        "    #vNew = vNew.at[0].set(jnp.maximum(vNew[0], 0.0))\n",
        "    #vNew[0] = jnp.maximum(vNew[0], 10**(-7))\n",
        "    #vNew[2] = jnp.maximum(vNew[2], 10**(-7))\n",
        "\n",
        "    riskNew = risk_infinity + jnp.sum(eigs_K * vNew[0])\n",
        "    return (vNew, riskNew), risk #(risk, vNew[0])\n",
        "\n",
        "  _, risks = jax.lax.scan(odeUpdate,(jnp.array([rho_init,sigma_init, chi_init]),risk_init),times)\n",
        "  return jnp.exp(times), risks"
      ],
      "metadata": {
        "id": "-F047aNh7LYI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the approximate ODE, i.e., it is an ODE where we drop some of the lower terms in the ODE. This should only be used when **batch size = 1**. It is a further approximation of the ODE that is solved in the method ode_resolvent_log_implicit_full."
      ],
      "metadata": {
        "id": "-qAvl8ve7Nk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ode_resolvent_log_implicit_approximate(eigs_K, rho_init, chi_init, sigma_init, risk_infinity,\n",
        "                  g1, g2, g3, delta, batch, D, t_max, Dt):\n",
        "  \"\"\"Generate the theoretical solution to momentum\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  eigs_K : array d\n",
        "      eigenvalues of covariance matrix (W^TDW)\n",
        "  rho_init : array d\n",
        "    initial rho_j's\n",
        "  chi_init : array (d)\n",
        "      initialization of chi's\n",
        "  sigma_init : array (d)\n",
        "      initialization of sigma's\n",
        "  risk_infinity : scalar\n",
        "      represents the risk value at time infinity\n",
        "  WtranD : array (v x d)\n",
        "      WtranD where D = diag(j^(-2alpha)) and W is the random matrix\n",
        "  alpha : float\n",
        "      data complexity\n",
        "  V : float\n",
        "      vocabulary size\n",
        "  g1, g2, g3 : function(time)\n",
        "      learning rate functions\n",
        "  delta : function(time)\n",
        "      momentum function\n",
        "  batch : int\n",
        "      batch size\n",
        "  D : int\n",
        "      number of eigenvalues (i.e. shape of eigs_K)\n",
        "  t_max : float\n",
        "      The number of epochs\n",
        "  Dt : float\n",
        "      time step used in Euler\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  t_grid: numpy.array(float)\n",
        "      the time steps used, which will discretize (0,t_max) into n_grid points\n",
        "  risks: numpy.array(float)\n",
        "      the values of the risk\n",
        "\n",
        "  \"\"\"\n",
        "  #times = jnp.arange(0, t_max, step = Dt, dtype= jnp.float64)\n",
        "  times = jnp.arange(0, jnp.log(t_max), step = Dt, dtype= jnp.float32)\n",
        "\n",
        "  risk_init = risk_infinity + jnp.sum(eigs_K * rho_init)\n",
        "\n",
        "  def inverse_3x3(Omega):\n",
        "      # Extract matrix elements\n",
        "      a11, a12, a13 = Omega[0][0], Omega[0][1], Omega[0][2]\n",
        "      a21, a22, a23 = Omega[1][0], Omega[1][1], Omega[1][2]\n",
        "      a31, a32, a33 = Omega[2][0], Omega[2][1], Omega[2][2]\n",
        "\n",
        "      # Calculate determinant\n",
        "      det = (a11*a22*a33 + a12*a23*a31 + a13*a21*a32\n",
        "            - a13*a22*a31 - a11*a23*a32 - a12*a21*a33)\n",
        "\n",
        "      #if abs(det) < 1e-10:\n",
        "      #    raise ValueError(\"Matrix is singular or nearly singular\")\n",
        "\n",
        "      # Calculate each element of inverse matrix\n",
        "      inv = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "\n",
        "      inv[0][0] = (a22*a33 - a23*a32) / det\n",
        "      inv[0][1] = (a13*a32 - a12*a33) / det\n",
        "      inv[0][2] = (a12*a23 - a13*a22) / det\n",
        "\n",
        "      inv[1][0] = (a23*a31 - a21*a33) / det\n",
        "      inv[1][1] = (a11*a33 - a13*a31) / det\n",
        "      inv[1][2] = (a13*a21 - a11*a23) / det\n",
        "\n",
        "      inv[2][0] = (a21*a32 - a22*a31) / det\n",
        "      inv[2][1] = (a12*a31 - a11*a32) / det\n",
        "      inv[2][2] = (a11*a22 - a12*a21) / det\n",
        "\n",
        "      return jnp.array(inv)\n",
        "\n",
        "  def odeApproximateUpdate(stuff, time):\n",
        "    v, risk = stuff\n",
        "    timePlus = jnp.exp(time + Dt)\n",
        "\n",
        "    Omega11 = -2.0 * batch * g2(timePlus) * eigs_K\n",
        "    Omega12 = 0.0 * jnp.ones_like(eigs_K)\n",
        "    Omega13 = 2.0 * g3(timePlus) * -1.0 * jnp.ones_like(eigs_K)\n",
        "    Omega1 = jnp.array([Omega11, Omega12, Omega13])\n",
        "\n",
        "    Omega21 = 0.0 * jnp.ones_like(eigs_K)\n",
        "    Omega22 = ( -2.0 * delta(timePlus) ) * jnp.ones_like(eigs_K)\n",
        "    Omega23 = 2.0 * g1(timePlus) * eigs_K * batch\n",
        "    Omega2 = jnp.array([Omega21, Omega22, Omega23])\n",
        "\n",
        "    Omega31 = g1(timePlus) * batch * eigs_K\n",
        "    Omega32 = -g3(timePlus) * jnp.ones_like(eigs_K)\n",
        "    Omega33 = -delta(timePlus) - g2(timePlus) * batch * eigs_K\n",
        "    Omega3 = jnp.array([Omega31, Omega32, Omega33])\n",
        "\n",
        "    Omega = jnp.array([Omega1, Omega2, Omega3]) #3 x 3 x d\n",
        "\n",
        "    Identity = jnp.tensordot( jnp.eye(3), jnp.ones(D), 0 )\n",
        "\n",
        "    A = inverse_3x3(Identity - (Dt * timePlus) * Omega) #3 x 3 x d\n",
        "\n",
        "    Gamma = jnp.array([batch * g2(timePlus)**2, batch * g1(timePlus)**2, 0.0])\n",
        "    z = jnp.einsum('i, j -> ij', jnp.array([1.0, 0.0, 0.0]), eigs_K)\n",
        "    G_Lambda = jnp.einsum('i,j->ij', Gamma, eigs_K) #3 x d\n",
        "\n",
        "    x_temp = v + Dt * timePlus * risk_infinity * G_Lambda\n",
        "    x = jnp.einsum('ijk, jk -> ik', A, x_temp)\n",
        "\n",
        "    y = jnp.einsum('ijk, jk -> ik', A, G_Lambda)\n",
        "\n",
        "    vNew = x + ( Dt * timePlus * y * jnp.sum(x * z) / (1.0 - Dt * timePlus * jnp.sum(y * z)) )\n",
        "    #vNew = vNew.at[0].set(jnp.maximum(vNew[0], 0.0))\n",
        "    #vNew[0] = jnp.maximum(vNew[0], 10**(-7))\n",
        "    #vNew[2] = jnp.maximum(vNew[2], 10**(-7))\n",
        "\n",
        "    riskNew = risk_infinity + jnp.sum(eigs_K * vNew[0])\n",
        "    return (vNew, riskNew), risk #(risk, vNew[0])\n",
        "\n",
        "  _, risks = jax.lax.scan(odeApproximateUpdate,(jnp.array([rho_init,sigma_init, chi_init]),risk_init),times)\n",
        "  return jnp.exp(times), risks"
      ],
      "metadata": {
        "id": "SQ-mAEbk7PrS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory for limit level of loss and left spectral edge"
      ],
      "metadata": {
        "id": "YFZPkHt57ld9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tt_lmin(alpha):\n",
        "    \"\"\"Generate left edge of the spectral measure (not accurate and only for alpha > 0.5)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        parameter of the model, ASSUMES V>D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theoretical prediction for the norm\n",
        "    \"\"\"\n",
        "\n",
        "    TMAX = 1000.0\n",
        "    c, _ = sp.integrate.quad(lambda x: 1.0/(1.0+x**(2*alpha)),0.0,TMAX)\n",
        "\n",
        "    return (1/(2*alpha-1))*((2*alpha/(2*alpha-1)/c)**(-2*alpha))"
      ],
      "metadata": {
        "id": "LmRa1_lA7uh9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tt_dbetacirc_VD(alpha, beta,V,D):\n",
        "    \"\"\"Generate the 'exact' finite V, D expression for $D^{1/2}\\circ{\\beta}$.\n",
        "    This is accurate for alpha.\n",
        "\n",
        "    This generates the finite V,D expression for the residual level (risk at time infinity)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha,beta : floats\n",
        "        parameters of the model, ASSUMES V>D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theoretical prediction for the norm\n",
        "    \"\"\"\n",
        "\n",
        "    cstar = 0.0\n",
        "    if 2*alpha >= 1.0:\n",
        "        kappa = tt_kappa_VD(alpha,V,D)\n",
        "        cstar = jnp.sum( jnp.arange(1,V,1.0)**(-2.0*(beta+alpha))/( jnp.arange(1,V,1.0)**(-2.0*(alpha))*kappa*(D**(2*alpha)) + 1.0))\n",
        "\n",
        "    if 2*alpha < 1.0:\n",
        "        #tau = D/jnp.sum( jnp.arange(1,V,1.0)**(-2.0*alpha))\n",
        "        tau = tt_tau_VD(alpha,V,D)\n",
        "        cstar = jnp.sum( jnp.arange(1,V,1.0)**(-2.0*(beta+alpha))/( jnp.arange(1,V,1.0)**(-2.0*(alpha))*tau + 1.0))\n",
        "\n",
        "\n",
        "    return cstar\n"
      ],
      "metadata": {
        "id": "_2svH9Cj7vRs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tt_kappa_VD(alpha, V,D):\n",
        "    \"\"\"Generate coefficient kappa with finite sample corrections.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        parameter of the model.\n",
        "    V,D : integers\n",
        "        parameters of the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theoretical prediction for kappa parameter\n",
        "    \"\"\"\n",
        "\n",
        "    TMAX = 1000.0\n",
        "    c, _ = sp.integrate.quad(lambda x: 1.0/(1.0+x**(2*alpha)),0.0,TMAX)\n",
        "    kappa=c**(-2.0*alpha)\n",
        "\n",
        "    kappa_it = lambda k : sp.integrate.quad(lambda x: 1.0/(k+x**(2*alpha)),0.0,V/D)[0]\n",
        "    eps = 10E-4\n",
        "    error = 1.0\n",
        "    while error > eps:\n",
        "        kappa1 = 1.0/kappa_it(kappa)\n",
        "        error = abs(kappa1/kappa - 1.0)\n",
        "        kappa = kappa1\n",
        "    return kappa"
      ],
      "metadata": {
        "id": "GybDiy5S7wlu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tt_tau_VD(alpha, V,D):\n",
        "    \"\"\"Generate coefficient tau with finite sample corrections.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        parameter of the model.\n",
        "    V,D : integers\n",
        "        parameters of the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theoretical prediction for kappa parameter\n",
        "    \"\"\"\n",
        "\n",
        "    tau_it = lambda k : jnp.sum( 1.0/(D*(jnp.arange(1,V,1)**(2*alpha) +k)))\n",
        "    tau = tau_it(0)\n",
        "    eps = 10E-4\n",
        "    error = 1.0\n",
        "    while error > eps:\n",
        "        tau1 = 1.0/tau_it(tau)\n",
        "        error = abs(tau1/tau - 1.0)\n",
        "        tau = tau1\n",
        "    return tau"
      ],
      "metadata": {
        "id": "SKBdT2zQ7x8K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton Theory (Elliot)\n",
        "\n",
        "Solves for the spectral distribution weighted by $D^{1/2} b$,\n",
        "$$\n",
        "\\langle (\\hat{K}-z)^{-1}, D^{1/2} b\\rangle,\n",
        "$$\n",
        "using **Newton Method**. It removes the point mass at $0$ from the spectrum. You will need to add this quantity back in.\n",
        "\n",
        "*Note this algorithm is much faster than the fixed point iteration above"
      ],
      "metadata": {
        "id": "w2uDpHGz72C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jax_gen_m_batched(v,d, alpha, xs,\n",
        "                eta = -6,\n",
        "                eta0 = 6.0,\n",
        "                etasteps=50,\n",
        "                batches = 100,\n",
        "                zbatch=1000):\n",
        "    \"\"\"Generate the powerlaw m by newton's method\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    v,d,alpha : floats\n",
        "        parameters of the model\n",
        "    xs : vector\n",
        "        The vector of x-positions at which to estimate the spectrum.  Complex is also possible.\n",
        "    eta : float\n",
        "        Error tolerance\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    m_Lambda: vector\n",
        "        m_Lambda evaluated at xs.\n",
        "    \"\"\"\n",
        "    #if zbatch > 0:\n",
        "    #    xsplit = jnp.split(xs,jnp.arange(1,len(xs)//zbatch,1)*zbatch)\n",
        "    #    ms = jnp.concatenate( [jax_gen_m_batched(v,d,alpha,x,eta,eta0,etasteps,batches,zbatch=0) for x in xsplit] )\n",
        "    #    return ms\n",
        "    v=jnp.int32(v)\n",
        "    d=jnp.complex64(d)\n",
        "    xs=jnp.complex64(xs)\n",
        "    xsplit = jnp.split(xs,jnp.arange(1,len(xs)//zbatch,1)*zbatch)\n",
        "\n",
        "\n",
        "    #print(\"xs length = {}\".format(len(xs)))\n",
        "\n",
        "    js=jnp.arange(1,v+1,1,dtype=jnp.complex64)**(-2.0*alpha)\n",
        "    jt=jnp.reshape(js,(batches,-1))\n",
        "    onesjtslice=jnp.ones_like(jt)[0]\n",
        "\n",
        "    # One Newton's method update step for current estimate m on a single value of z\n",
        "    def mup_single(m,z):\n",
        "        m1 = m\n",
        "        F=m1\n",
        "        Fprime=jnp.ones_like(m1,dtype=jnp.complex64)\n",
        "        for j in range(batches):\n",
        "            denom = (jnp.outer(jt[j],m1) - jnp.outer(onesjtslice,z))\n",
        "            F += (1.0/d)*jnp.sum(jnp.outer(jt[j],m1)/denom,axis=0)\n",
        "            Fprime -= (1.0/d)*jnp.sum(jnp.outer(jt[j],z)/(denom**2),axis=0)\n",
        "        return (-F + 1.0)/Fprime + m1\n",
        "        #return 0.1*jnp.where(mask, m1, newm1)+0.9*m1\n",
        "\n",
        "#    mup_single = jax.jit(mup_single, static_argnums=(0,1))\n",
        "\n",
        "    def mup_scanner(ms,z,x):\n",
        "        #mups = lambda m : mup_single(m,z*1.0j+xs)\n",
        "        return mup_single(ms,z*1.0j+x), False\n",
        "\n",
        "    #mup_scanner = jax.jit(mup_scanner, static_argnums=(0,1))\n",
        "    mup_scannerjit =  jax.jit(mup_scanner)\n",
        "\n",
        "    etas = jnp.logspace(eta0,eta,num = etasteps)\n",
        "    ms = jnp.concatenate( [jax.lax.scan(lambda m,z: mup_scanner(m,z,x),jnp.ones_like(x, dtype = jnp.complex64),etas)[0] for x in xsplit] )\n",
        "    #ms, _ = jax.lax.scan(mup_scanner,jnp.ones_like(xs),etas)\n",
        "\n",
        "    return ms"
      ],
      "metadata": {
        "id": "6EmxXltD73UX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jax_gen_trace_fmeasure(v,d, alpha, beta, xs,\n",
        "                err = -6.0, timeChecks = False, batches=100):\n",
        "    \"\"\"Generate the trace resolvent\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    v,d,alpha,beta : floats\n",
        "        parameters of the model\n",
        "    xs : floats\n",
        "        X-values at which to return the trace-resolvent\n",
        "    err : float\n",
        "        Error tolerance, log scale\n",
        "    timeChecks: bool\n",
        "        Print times for each part\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Volterra: vector\n",
        "        values of the solution of the Volterra\n",
        "    \"\"\"\n",
        "\n",
        "    eps = 10.0**(err)\n",
        "\n",
        "    zs = xs + 1.0j*eps\n",
        "\n",
        "    if timeChecks:\n",
        "        print(\"The number of points on the spectral curve is {}\".format(len(xs)))\n",
        "\n",
        "    eta = jnp.log10(eps*(d**(-2*alpha)))\n",
        "    eta0 = 6\n",
        "    etasteps = jnp.int32(40 + 10*(2*alpha)*jnp.log(d))\n",
        "\n",
        "    start=time.time()\n",
        "    if timeChecks:\n",
        "        print(\"Running the Newton generator with {} steps\".format(etasteps))\n",
        "\n",
        "    ms = jax_gen_m_batched(v,d,alpha,zs,eta,eta0,etasteps,batches)\n",
        "\n",
        "    end = time.time()\n",
        "    if timeChecks:\n",
        "        print(\"Completed Newton in {} time\".format(end-start) )\n",
        "    start = end\n",
        "\n",
        "    js=jnp.arange(1,v+1,1)**(-2.0*alpha)\n",
        "    jbs=jnp.arange(1,v+1,1)**(-2.0*(alpha+beta))\n",
        "\n",
        "    jt=jnp.reshape(js,(batches,-1))\n",
        "    jbt=jnp.expand_dims(jnp.reshape(jbs,(batches,-1)),-1)\n",
        "    onesjtslice=jnp.ones_like(jt)[0]\n",
        "\n",
        "    Fmeasure = jnp.zeros_like(ms)\n",
        "    Kmeasure = jnp.zeros_like(ms)\n",
        "\n",
        "    for j in range(batches):\n",
        "        Fmeasure += jnp.sum(jbt[j]/(jnp.outer(jt[j],ms) - jnp.outer(onesjtslice,zs + 1.0j*(10**eta))),axis=0)\n",
        "        Kmeasure += jnp.sum(1.0/(jnp.outer(jt[j],ms) - jnp.outer(onesjtslice,zs + 1.0j*(10**eta))),axis=0)\n",
        "\n",
        "    #Kmeasure = (1-ms)*((zs + 1.0j*(10**eta)))*d\n",
        "\n",
        "    #Fmeasure = Fmeasure * dzs / (jnp.pi)\n",
        "    #Kmeasure = Kmeasure * dzs / (jnp.pi)\n",
        "\n",
        "    return jnp.imag(Fmeasure/zs) / jnp.pi\n",
        "    #return jnp.imag(Fmeasure/(zs**2)) / jnp.pi"
      ],
      "metadata": {
        "id": "LP5uxqfu74kB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing initial $\\rho_j$'s deterministically"
      ],
      "metadata": {
        "id": "cjdWXxru76FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generate the initial rho_j's deterministically.\n",
        "\n",
        "This performs many small contour integrals each surrounding the real eigenvalues\n",
        "where the vector a contains the values for the lower (left) edges of the\n",
        "contours and the vector b contains the values of the upper (right) edges of the\n",
        "contours.\n",
        "\n",
        "The quantity we want to calculate is these contour integrals over the density\n",
        "of zs, but we are choosing the xs to discretize this density. We therefore need\n",
        "to choose the xs to be in a fine enough grid to give the desired accuracy.\n",
        "\n",
        "This code uses a hacky method to choose the xs where the eigenvalues are divided\n",
        "into num_splits different chunks (each containing the same num of eigenvalues)\n",
        "so that the range of x values spanned is large for the large eigenvalues and\n",
        "small for the small eigenvalues. Then this uses a linearly spaced grid within\n",
        "each split so that each split uses the same number of xs.\n",
        "\n",
        "The smallest eigenvalues actually don't need this dense of a grid, because they\n",
        "make very small contributions, and the largest eigenvalues don't need this dense\n",
        "of a grid because they are far apart. It is actually the intermediate\n",
        "eigenvalues that are tricky because they are close together but still contribute\n",
        "significantly.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "num_splits (int): number of splits\n",
        "a (vector): lower values of z's to be used to compute the density starting\n",
        "            from largest j^{-2alpha} to smallest j^{-2alpha}\n",
        "b (vector): upper values of z's to be used to compute the density starting from\n",
        "            largest j^{-2alpha} to smallest j^{-2alpha}\n",
        "xs_per_split (int): the number of x values to use per split\n",
        "\n",
        "Returns\n",
        "-------\n",
        "rho_weights: vector\n",
        "    returns rho_j weights in order of largest j^{-2alpha} to smallest j^{-2alpha}\n",
        "\"\"\"\n",
        "\n",
        "def weights(xs, density, a, b):\n",
        "    # Compute integrals\n",
        "      integrals = []\n",
        "      def theoretical_integral(lower, upper):\n",
        "        # Normalize density to make it a probability measure\n",
        "        dx = xs[1] - xs[0]\n",
        "        #norm = jnp.sum(density) * dx\n",
        "        #density = density / norm\n",
        "\n",
        "        # Find indices corresponding to interval [a,b]\n",
        "        idx = (xs >= lower) & (xs <= upper)\n",
        "        integral = jnp.sum(density[idx]) * dx\n",
        "        return float(integral)\n",
        "      i = 0\n",
        "      for lower, upper in zip(a,b):\n",
        "        integrals.append(theoretical_integral(lower, upper))\n",
        "        #integrals.at[i].set(theoretical_integral(lower, upper))\n",
        "        i = i+ 1\n",
        "      return integrals\n",
        "\n",
        "\n",
        "def deterministic_rho_weights(num_splits, a, b, xs_per_split = 10000):\n",
        "  a_splits = jnp.split(a, num_splits)\n",
        "  b_splits = jnp.split(b, num_splits)\n",
        "\n",
        "  # Vectorize lower and upper bounds\n",
        "  lower_bounds = jnp.array([jnp.min(split) for split in a_splits])\n",
        "  upper_bounds = jnp.array([jnp.max(split) for split in b_splits])\n",
        "\n",
        "  # Generate xs and zs for all splits\n",
        "  xs = jnp.vstack([jnp.linspace(lower, upper, xs_per_split) for lower, upper in zip(lower_bounds, upper_bounds)])\n",
        "  zs = xs.astype(jnp.complex64)\n",
        "\n",
        "  rho_weights = jnp.array([])\n",
        "  for a_split, b_split in zip(a_splits, b_splits):\n",
        "    lower_bound_split = jnp.min(a_split)\n",
        "    upper_bound_split = jnp.max(b_split)\n",
        "    xs = jnp.linspace(lower_bound_split, upper_bound_split, xs_per_split)\n",
        "    err = -10\n",
        "    batches = 1\n",
        "\n",
        "    zs = xs.astype(jnp.complex64)\n",
        "    density = jax_gen_trace_fmeasure(V, D, alpha, beta, zs, err=err, batches = batches)\n",
        "\n",
        "    rho_weights_split = weights(xs, density, a_split, b_split)\n",
        "    rho_weights_split = jnp.array(rho_weights_split)\n",
        "    rho_weights = jnp.concatenate([rho_weights, rho_weights_split], axis=0)\n",
        "\n",
        "\n",
        "  # Compute density for all splits\n",
        "  #density = jax.vmap(lambda z: jax_gen_trace_fmeasure(V, D, alpha, beta, z, err=-10, batches=1))(zs)\n",
        "\n",
        "  # Compute rho_weights for all splits\n",
        "  #rho_weights = jnp.array([weights(x, d, a_s, b_s) for x, d, a_s, b_s in zip(xs, density, a_splits, b_splits)])\n",
        "\n",
        "  # Flatten the rho_weights\n",
        "  #rho_weights = rho_weights.flatten()\n",
        "\n",
        "  return rho_weights"
      ],
      "metadata": {
        "id": "MkGKYh9S77aD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run SGD and ODE Solver"
      ],
      "metadata": {
        "id": "u_Ty96Lo792j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha= 1.0\n",
        "beta= 0.4\n",
        "eta = 0.0\n",
        "\n",
        "#SGD steps\n",
        "sgd_steps = 10**1\n",
        "delta_constant = jnp.maximum(2.0  + ( 2.0 * beta - 1 ) / (alpha ), 2.0 - 1.0 / alpha) + 1.0 #Need to be bigger than 2 + (2 * beta - 1) / (2 * alpha)\n",
        "print('delta is {:.2f}'.format(delta_constant))\n",
        "\n",
        "D = 500\n",
        "V = 5 * D\n",
        "sgd_batch = 1 #jnp.int32(0.2*D)\n",
        "\n",
        "omega = 1.0/jnp.float32(D)\n",
        "traceK = jnp.sum(jnp.arange(1,V+1,dtype=jnp.float32)**(-2*alpha))\n",
        "print('traceK is {:.2f}'.format(traceK))\n",
        "\n",
        "key,nkey = jax.random.split(key)\n",
        "W = jnp.sqrt(omega)*jax.random.normal(nkey, (V,D))\n",
        "data_scale = jnp.power(jnp.arange(1,V+1,dtype=jnp.float32),-1.0*alpha) #D^(1/2)\n",
        "\n",
        "#move power-scaling from X's to beta and W to save computation\n",
        "check_beta = jnp.power(jnp.arange(1,V+1,dtype=jnp.float32),-1.0*(beta+alpha)) #D^(1/2) b\n",
        "check_beta = jnp.reshape(check_beta,(V,1))\n",
        "\n",
        "check_W1 = jnp.einsum('i, ij->ij', data_scale, W) #D^(1/2) W\n",
        "WtranD = jnp.einsum('ij, i->ji', W, data_scale**2) #WtranD\n",
        "check_W = jnp.reshape(data_scale,(V,1)) * W #D^(1/2) W\n",
        "\n",
        "hatK = jnp.einsum('ji,jk->ik', check_W1, check_W1)\n",
        "\n",
        "def ab_oracle(key,batch):\n",
        "  key, nkey = jax.random.split(key)\n",
        "  xs = jax.random.normal(nkey, (batch, V))\n",
        "  A = jnp.tensordot(xs,check_W,1)\n",
        "  key, nkey = jax.random.split(key)\n",
        "  noise = jax.random.normal(nkey,(batch, 1))\n",
        "  y = jnp.tensordot(xs, check_beta,1) + eta*noise\n",
        "  return A,y\n",
        "\n",
        "def square_loss(theta):\n",
        "  v=jnp.tensordot(check_W,theta,1) - check_beta\n",
        "  return jnp.sum(v*v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htx7KiK17_Bb",
        "outputId": "438353fb-3a8f-4fab-d352-670b4aa3bf4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta is 2.80\n",
            "traceK is 1.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD Code"
      ],
      "metadata": {
        "id": "m0SKHKH88Ame"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantities need for empirical ODE check"
      ],
      "metadata": {
        "id": "ZWLSku368CO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for running pure SGD\n",
        "sgd_gamma_2_constant = 0.5\n",
        "sgd_gamma_2_scaling = sgd_gamma_2_constant * jnp.minimum( 1.0 / jnp.float32(sgd_batch), 1.0 / traceK )\n",
        "def g2_sgd(time):\n",
        "  return sgd_gamma_2_scaling * jnp.ones_like(time)\n",
        "def g1_sgd(time):\n",
        "  return 0.0\n",
        "def g3_sgd(time):\n",
        "  return 0.0\n",
        "def delta_sgd(time):\n",
        "  return 0.0\n",
        "\n",
        "key,nkey = jax.random.split(key)\n",
        "\n",
        "losses_sgd, times_sgd, theta_final = jax_lsq_momentum1_opt2(nkey,\n",
        "                g1_sgd, g2_sgd, g3_sgd, delta_sgd, sgd_batch, sgd_steps, jnp.zeros(D),jnp.zeros(D),\n",
        "                ab_oracle,square_loss\n",
        "                )\n",
        "\n",
        "print('Initial loss value is {}'.format( losses_sgd[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4K6FUv8CsR",
        "outputId": "88d2ba2f-7d0a-4b6e-f209-1256a27652e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss value is 1.2470312118530273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K_check method (Equation 18)\n",
        "# This is slow, use the K_hat cell below instead.\n",
        "\n",
        "Keigs, Kevecs = np.linalg.eigh(hatK)\n",
        "\n",
        "b = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*beta)\n",
        "\n",
        "hold_b = np.einsum('ij, j->i', WtranD,b)\n",
        "\n",
        "check_b = np.linalg.solve(hatK, hold_b)\n",
        "\n",
        "halfDW = np.einsum('i,ij->ij', data_scale, W)\n",
        "halfDb = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*(beta+alpha))\n",
        "\n",
        "riskInfty = np.linalg.lstsq(check_W, halfDb)[1]\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "print('Empirical limiting loss value is {}'.format(riskInfty[0]))\n",
        "print('Theoretical limiting loss value is {}'.format(tt_dbetacirc_VD(alpha, beta,V,D)))\n",
        "\n",
        "#Initialize the rhos\n",
        "initTheta = jnp.zeros(D, dtype=jnp.float32)\n",
        "initY = jnp.zeros(D, dtype=jnp.float32)\n",
        "rho = jnp.einsum('ij,i->j',  Kevecs.astype(jnp.float32), initTheta - check_b)\n",
        "rho_init = rho**2 #d each row is the ith eigenvalue\n",
        "rho_init.astype(jnp.float32)\n",
        "omegaY = jnp.einsum('ij, i -> j',  Kevecs.astype(jnp.float32), initY) #d\n",
        "sigma_init = omegaY**2\n",
        "chi_init = omegaY * rho\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * Keigs) + riskInfty))\n",
        "\n",
        "Dt = 10**(-2)\n",
        "\n",
        "#check_b = jnp.ravel(theta_final)\n",
        "\n",
        "odeTimes_sgd, odeRisks_sgd = ode_resolvent_log_implicit_full(Keigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInfty,\n",
        "                                               g1_sgd, g2_sgd, g3_sgd, delta_sgd, sgd_batch, D, sgd_steps, Dt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq02hNfh8FIE",
        "outputId": "050ec33c-6783-4c1f-cfed-fa56ecd6edfc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-719054b2356c>:15: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  riskInfty = np.linalg.lstsq(check_W, halfDb)[1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical limiting loss value is 8.853174949763343e-05\n",
            "Theoretical limiting loss value is 9.77418094407767e-05\n",
            "Initial loss value is [1.2470324]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This uses K_hat (instead of K_check) to compute rho_init (see Prop. F1).\n",
        "# Using K_hat is a much easier + faster way to compute rho_init than using K_check.\n",
        "# This one should work for all values of alpha (including alpha < 0.5) and should work for any batch size.\n",
        "\n",
        "# For the spectra computations\n",
        "Uvec, s, Vvec =jnp.linalg.svd(check_W,full_matrices=False)\n",
        "\n",
        "#Compute < ( D^1/2 W W^T D^(1/2) - z)^{-1}, D^(1/2) b >\n",
        "check_beta_weight = jnp.tensordot(check_beta,Uvec,axes=[[0],[0]])[0]\n",
        "\n",
        "rho_init = (check_beta_weight)**2 / s**2\n",
        "rho_init.astype(jnp.float32)\n",
        "sigma_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "K_eigs = s**2\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * K_eigs) + riskInftyTheory))\n",
        "\n",
        "Dt = 10**(-2)\n",
        "\n",
        "odeTimes_sgd_1, odeRisks_sgd_1 = ode_resolvent_log_implicit_full(K_eigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_sgd, g2_sgd, g3_sgd, delta_sgd, sgd_batch, D, sgd_steps, Dt)\n"
      ],
      "metadata": {
        "id": "bSawCdRr8GQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Theory spectra generated by Newton (Elliot)\n",
        "\n",
        "#Compute the theoretical limiting loss value\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "\n",
        "# Compute theoretical integrals using density approximation\n",
        "lower_bound = tt_lmin(alpha)*(D**(-2*alpha))#jnp.minimum(tt_lmin(alpha)*(D**(-2*alpha)), 0.9*(D+1)**(-2.0*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha)) #tt_lmin(alpha)*(D**(-2*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha))\n",
        "upper_bound = 1.0*1.1\n",
        "\n",
        "fake_eigs = np.power(np.arange(1,D+1,dtype=jnp.float32),-2.0*alpha)\n",
        "b_values = fake_eigs - 0.5 * jnp.diff(fake_eigs, prepend = upper_bound)\n",
        "a_values = fake_eigs + 0.5 * jnp.diff(fake_eigs, append = lower_bound)\n",
        "num_splits = 5 #Must divide D into equal parts\n",
        "rho_weights = deterministic_rho_weights(num_splits, a_values, b_values)\n",
        "\n",
        "#rho_weights = split_eigenvalues(num_splits, a_values, b_values)\n",
        "#rho_weights = weights(xs, density, a_values, b_values)\n",
        "\n",
        "print('Initial loss value is is {}'.format(jnp.sum( rho_weights*fake_eigs) + riskInftyTheory))\n",
        "\n",
        "# Compute integrals\n",
        "\n",
        "\n",
        "#dx = xs[1] - xs[0]\n",
        "\n",
        "rho_init = rho_weights #density * dx\n",
        "#num_grid_points = jnp.shape(xs)[0] #Represents the number of eigenvalues\n",
        "num_grid_points = D\n",
        "sigma_init = jnp.zeros(num_grid_points, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(num_grid_points, dtype=jnp.float32)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b1mUGR58IxV",
        "outputId": "e50e30ac-da43-412e-e1d9-644c2e7f8ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical limiting loss value is 9.77418094407767e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dt = 10**(-2) #10**(-2)\n",
        "\n",
        "odeTimes_sgd_theory, odeRisks_sgd_theory = ode_resolvent_log_implicit_full(fake_eigs, rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_sgd, g2_sgd, g3_sgd, delta_sgd, sgd_batch, num_grid_points, sgd_steps, Dt)\n"
      ],
      "metadata": {
        "id": "OalFDt_18Jgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Theory spectra generated by Newton (Elliot)\n",
        "\n",
        "#Compute the theoretical limiting loss value\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "\n",
        "# Compute theoretical integrals using density approximation\n",
        "lower_bound = tt_lmin(alpha)*(D**(-2*alpha))#jnp.minimum(tt_lmin(alpha)*(D**(-2*alpha)), 0.9*(D+1)**(-2.0*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha)) #tt_lmin(alpha)*(D**(-2*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha))\n",
        "upper_bound = 1.0*1.1\n",
        "\n",
        "fake_eigs = np.power(np.arange(1,D+1,dtype=jnp.float32),-2.0*alpha)\n",
        "b_values = fake_eigs - 0.5 * jnp.diff(fake_eigs, prepend = upper_bound)\n",
        "a_values = fake_eigs + 0.5 * jnp.diff(fake_eigs, append = lower_bound)\n",
        "num_splits = 5 #Must divide D into equal parts\n",
        "rho_weights = deterministic_rho_weights(num_splits, a_values, b_values)\n",
        "\n",
        "#rho_weights = split_eigenvalues(num_splits, a_values, b_values)\n",
        "#rho_weights = weights(xs, density, a_values, b_values)\n",
        "\n",
        "print('Initial loss value is is {}'.format(jnp.sum( rho_weights*fake_eigs) + riskInftyTheory))\n",
        "\n",
        "# Compute integrals\n",
        "\n",
        "\n",
        "#dx = xs[1] - xs[0]\n",
        "\n",
        "rho_init = rho_weights #density * dx\n",
        "#num_grid_points = jnp.shape(xs)[0] #Represents the number of eigenvalues\n",
        "num_grid_points = D\n",
        "sigma_init = jnp.zeros(num_grid_points, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(num_grid_points, dtype=jnp.float32)"
      ],
      "metadata": {
        "id": "zrmzcLfK8LwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dt = 10**(-2) #10**(-2)\n",
        "\n",
        "odeTimes_sgd_theory_approximate, odeRisks_sgd_theory_approximate = ode_resolvent_log_implicit_approximate(fake_eigs, rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_sgd, g2_sgd, g3_sgd, delta_sgd, sgd_batch, num_grid_points, sgd_steps, Dt)\n"
      ],
      "metadata": {
        "id": "iGtwb6AV8N01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dana-constant Code"
      ],
      "metadata": {
        "id": "HljyLfS-8OWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for running dana with constant learning rate\n",
        "dana_gamma_3_constant = 0.1\n",
        "dana_gamma_3_scaling =  dana_gamma_3_constant / ( float(D) ) *  1.0 / traceK\n",
        "dana_gamma_2_scaling = 0.5 * jnp.minimum( 1.0 / jnp.float32(sgd_batch), 1.0 / traceK ) #0.5 / traceK\n",
        "dana_gamma_1_scaling = 1.0\n",
        "dana_expMomentum = 1.0\n",
        "\n",
        "def g2_dana(time):\n",
        "  return dana_gamma_2_scaling * jnp.ones_like(time)\n",
        "\n",
        "def g1_dana(time):\n",
        "  return dana_gamma_1_scaling * jnp.ones_like(time)\n",
        "\n",
        "def g3_dana(time):\n",
        "  return dana_gamma_3_scaling * jnp.ones_like(time)\n",
        "\n",
        "def delta_dana(time):\n",
        "  return delta_constant / ( (1.0 + time)**dana_expMomentum ) * jnp.ones_like(time)\n",
        "\n",
        "key,nkey = jax.random.split(key)\n",
        "\n",
        "losses_DANA,times_DANA, theta_final_DANA = jax_lsq_momentum1_opt2(nkey,\n",
        "                g1_dana, g2_dana, g3_dana, delta_dana, sgd_batch, sgd_steps, jnp.zeros(D),jnp.zeros(D),\n",
        "                ab_oracle,square_loss\n",
        "                )\n",
        "\n",
        "print('Initial loss value is {}'.format( losses_DANA[0]) )"
      ],
      "metadata": {
        "id": "Ht06w4dC8RAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Keigs, Kevecs = np.linalg.eigh(hatK)\n",
        "\n",
        "b = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*beta)\n",
        "\n",
        "hold_b = np.einsum('ij, j->i', WtranD,b)\n",
        "\n",
        "check_b = np.linalg.solve(hatK, hold_b)\n",
        "\n",
        "halfDW = np.einsum('i,ij->ij', data_scale, W)\n",
        "halfDb = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*(beta+alpha))\n",
        "\n",
        "riskInfty = np.linalg.lstsq(check_W, halfDb)[1]\n",
        "print('Empirical limiting loss value is {}'.format(riskInfty[0]))\n",
        "print('Theoretical limiting loss value is {}'.format(tt_dbetacirc_VD(alpha, beta,V,D)))\n",
        "\n",
        "#Initialize the rhos\n",
        "initTheta = jnp.zeros(D, dtype=jnp.float32)\n",
        "initY = jnp.zeros(D, dtype=jnp.float32)\n",
        "rho = jnp.einsum('ij,i->j',  Kevecs.astype(jnp.float32), initTheta - check_b)\n",
        "rho_init = rho**2 #d each row is the ith eigenvalue\n",
        "rho_init.astype(jnp.float32)\n",
        "omegaY = jnp.einsum('ij, i -> j',  Kevecs.astype(jnp.float32), initY) #d\n",
        "sigma_init = omegaY**2\n",
        "chi_init = omegaY * rho\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * Keigs) + riskInfty))\n",
        "\n",
        "Dt = 10**(-2)\n",
        "\n",
        "odeTimes_dana, odeRisks_dana = ode_resolvent_log_implicit_full(Keigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInfty,\n",
        "                                               g1_dana, g2_dana, g3_dana, delta_dana, sgd_batch, D, sgd_steps, Dt)"
      ],
      "metadata": {
        "id": "jS6-EwGS8UcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the spectra computations\n",
        "Uvec, s, Vvec =jnp.linalg.svd(check_W,full_matrices=False)\n",
        "\n",
        "#Compute < ( D^1/2 W W^T D^(1/2) - z)^{-1}, D^(1/2) hat{\\beta} >\n",
        "check_beta_weight = jnp.tensordot(check_beta,Uvec,axes=[[0],[0]])[0]\n",
        "\n",
        "rho_init = (check_beta_weight)**2 / s**2\n",
        "rho_init.astype(jnp.float32)\n",
        "sigma_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "K_eigs = s**2\n",
        "\n",
        "Dt = 10**(-2)\n",
        "\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * K_eigs) + riskInftyTheory))\n",
        "\n",
        "odeTimes_dana_1, odeRisks_dana_1 = ode_resolvent_log_implicit_full(K_eigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_dana, g2_dana, g3_dana, delta_dana, sgd_batch, D, sgd_steps, Dt)"
      ],
      "metadata": {
        "id": "saWnh15S8V8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dana-decay code"
      ],
      "metadata": {
        "id": "LcJdO-O-8dCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for running dana with decaying gamma 3 learning rate\n",
        "dana_gamma_3_constant_decay = 0.1\n",
        "dana_gamma_3_scaling_decay =  dana_gamma_3_constant_decay\n",
        "dana_gamma_2_scaling_decay = 0.5 * jnp.minimum( 1.0 / jnp.float32(sgd_batch), 1.0 / traceK ) #0.5 / traceK\n",
        "dana_gamma_1_scaling_decay = 1.0\n",
        "dana_expMomentum_decay = 1.0\n",
        "\n",
        "def g2_dana_decay(time):\n",
        "  return dana_gamma_2_scaling_decay * jnp.ones_like(time)\n",
        "\n",
        "def g1_dana_decay(time):\n",
        "  return dana_gamma_1_scaling_decay * jnp.ones_like(time)\n",
        "\n",
        "def g3_dana_decay(time):\n",
        "  return dana_gamma_3_scaling_decay / (1.0 + time)**( 1.0 / (2.0 * alpha) ) * 1.0 / traceK\n",
        "\n",
        "def delta_dana_decay(time):\n",
        "  return delta_constant / ( (1.0 + time)**dana_expMomentum_decay ) * jnp.ones_like(time)\n",
        "\n",
        "key,nkey = jax.random.split(key)\n",
        "\n",
        "losses_DANA_decay,times_DANA_decay, theta_final_DANA_decay = jax_lsq_momentum1_opt2(nkey,\n",
        "                g1_dana_decay, g2_dana_decay, g3_dana_decay, delta_dana_decay, sgd_batch, sgd_steps, jnp.zeros(D),jnp.zeros(D),\n",
        "                ab_oracle,square_loss\n",
        "                )\n",
        "\n",
        "print('Initial loss value is {}'.format( losses_DANA_decay[0]))"
      ],
      "metadata": {
        "id": "GiBlUl-Z8eW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Keigs, Kevecs = np.linalg.eigh(hatK)\n",
        "\n",
        "b = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*beta)\n",
        "\n",
        "hold_b = np.einsum('ij, j->i', WtranD,b)\n",
        "\n",
        "check_b = np.linalg.solve(hatK, hold_b)\n",
        "\n",
        "halfDW = np.einsum('i,ij->ij', data_scale, W)\n",
        "halfDb = np.power(np.arange(1,V+1,dtype=jnp.float32),-1.0*(beta+alpha))\n",
        "\n",
        "riskInfty = np.linalg.lstsq(check_W, halfDb)[1]\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "print('Empirical limiting loss value is {}'.format(riskInfty[0]))\n",
        "print('Theoretical limiting loss value is {}'.format(tt_dbetacirc_VD(alpha, beta,V,D)))\n",
        "\n",
        "#Initialize the rhos\n",
        "initTheta = jnp.zeros(D, dtype=jnp.float32)\n",
        "initY = jnp.zeros(D, dtype=jnp.float32)\n",
        "rho = jnp.einsum('ij,i->j',  Kevecs.astype(jnp.float32), initTheta - check_b)\n",
        "rho_init = rho**2 #d each row is the ith eigenvalue\n",
        "rho_init.astype(jnp.float32)\n",
        "omegaY = jnp.einsum('ij, i -> j',  Kevecs.astype(jnp.float32), initY) #d\n",
        "sigma_init = omegaY**2\n",
        "chi_init = omegaY * rho\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * Keigs) + riskInfty))\n",
        "\n",
        "Dt = 10**(-2)\n",
        "\n",
        "#check_b = jnp.ravel(theta_final)\n",
        "\n",
        "odeTimes_dana_decay, odeRisks_dana_decay = ode_resolvent_log_implicit_full(Keigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInfty,\n",
        "                                               g1_dana_decay, g2_dana_decay, g3_dana_decay, delta_dana_decay, sgd_batch, D, sgd_steps, Dt)"
      ],
      "metadata": {
        "id": "W13mpcGo8gnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the spectra computations\n",
        "Uvec, s, Vvec =jnp.linalg.svd(check_W,full_matrices=False)\n",
        "\n",
        "#Compute < ( D^1/2 W W^T D^(1/2) - z)^{-1}, D^(1/2) hat{\\beta} >\n",
        "check_beta_weight = jnp.tensordot(check_beta,Uvec,axes=[[0],[0]])[0]\n",
        "\n",
        "rho_init = (check_beta_weight)**2 / s**2\n",
        "rho_init.astype(jnp.float32)\n",
        "sigma_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(D, dtype=jnp.float32)\n",
        "\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "K_eigs = s**2\n",
        "\n",
        "print('Initial loss value is {}'.format( jnp.sum(rho_init * K_eigs) + riskInftyTheory))\n",
        "\n",
        "\n",
        "odeTimes_dana_decay_1, odeRisks_dana_decay_1 = ode_resolvent_log_implicit_full(K_eigs.astype(jnp.float32), rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_dana_decay, g2_dana_decay, g3_dana_decay, delta_dana_decay, sgd_batch, D, sgd_steps, Dt)"
      ],
      "metadata": {
        "id": "fjqYtMGv8hr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Theory spectra generated by Newton (Elliot)\n",
        "\n",
        "#Compute the theoretical limiting loss value\n",
        "riskInftyTheory = tt_dbetacirc_VD(alpha, beta,V,D)\n",
        "print('Theoretical limiting loss value is {}'.format(riskInftyTheory))\n",
        "\n",
        "# Compute theoretical integrals using density approximation\n",
        "lower_bound = tt_lmin(alpha)*(D**(-2*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha)) #tt_lmin(alpha)*(D**(-2*alpha)) #jnp.minimum(0.00001, 0.9*(D+1)**(-2.0*alpha))\n",
        "upper_bound = 1.0*1.1\n",
        "\n",
        "\n",
        "#xs = jnp.linspace(lower_bound, upper_bound, 10000)\n",
        "\n",
        "#err = -10\n",
        "#batches = 1\n",
        "#zs = xs.astype(jnp.complex64)\n",
        "\n",
        "#density = jax_gen_trace_fmeasure(V, D, alpha, beta, zs, err=err, batches = batches)\n",
        "\n",
        "fake_eigs = np.power(np.arange(1,D+1,dtype=jnp.float32),-2.0*alpha)\n",
        "b_values = fake_eigs - 0.5 * jnp.diff(fake_eigs, prepend = upper_bound)\n",
        "a_values = fake_eigs + 0.5 * jnp.diff(fake_eigs, append = lower_bound)\n",
        "#rho_weights = deterministic_rho_weights(xs, density, a_values, b_values)\n",
        "\n",
        "\n",
        "num_splits = 5\n",
        "rho_weights = deterministic_rho_weights(num_splits, a_values, b_values)\n",
        "\n",
        "print('Initial loss value is is {}'.format(jnp.sum( rho_weights*fake_eigs) + riskInftyTheory))\n",
        "\n",
        "# Compute integrals\n",
        "#dx = xs[1] - xs[0]\n",
        "\n",
        "rho_init = rho_weights #density * dx\n",
        "#num_grid_points = jnp.shape(xs)[0] #Represents the number of eigenvalues\n",
        "num_grid_points = D\n",
        "sigma_init = jnp.zeros(num_grid_points, dtype=jnp.float32)\n",
        "chi_init = jnp.zeros(num_grid_points, dtype=jnp.float32)\n",
        "\n",
        "Dt = 10**(-2) #10**(-2)\n",
        "\n",
        "\n",
        "odeTimes_dana_decay_theory, odeRisks_dana_decay_theory = ode_resolvent_log_implicit_full(fake_eigs, rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_dana_decay, g2_dana_decay, g3_dana_decay, delta_dana_decay, sgd_batch, num_grid_points, sgd_steps, Dt)\n"
      ],
      "metadata": {
        "id": "nKTAFV_z8ixN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odeTimes_dana_decay_theory_approximate, odeRisks_dana_decay_theory_approximate = ode_resolvent_log_implicit_approximate(fake_eigs, rho_init, chi_init, sigma_init, riskInftyTheory,\n",
        "                                               g1_dana_decay, g2_dana_decay, g3_dana_decay, delta_dana_decay, sgd_batch, num_grid_points, sgd_steps, Dt)"
      ],
      "metadata": {
        "id": "tFz6gG0u8kNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph"
      ],
      "metadata": {
        "id": "naYUnwZf8qSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "axarr = plt.axes()\n",
        "axarr.set_xlabel(\"flops\", fontsize = '28')\n",
        "axarr.set_ylabel(\"risk\", fontsize = '28')\n",
        "plt.title('alpha = {:,},'.format(alpha)+' beta = {:,}'.format(beta)+', d = {:,}'.format(D)+', batch = {:,}'.format(sgd_batch))\n",
        "\n",
        "plt.plot((times_sgd) * (float(D)),losses_sgd, label= 'sgd', linewidth = 3.0, c = 'blue', alpha = 0.2) #exponent gamma_1 = {:,f}'.format(exponent)+', gamma2={:,}'.format(scaling_gamma_2_momentum)+'/traceK', linewidth = 3.0)\n",
        "#plt.plot(odeTimes_sgd* (float(D)), odeRisks_sgd, label= 'ode (sgd, empirical old)', linewidth = 3.0, c = 'blue')\n",
        "plt.plot(odeTimes_sgd_1 * (float(D)), odeRisks_sgd_1, label= 'ode (sgd, empirical new)', linewidth = 3.0, c = 'blue', alpha = 0.6)\n",
        "#plt.plot(odeTimes_sgd_theory * (float(D)), odeRisks_sgd_theory, label= 'ode (sgd, deterministic)', linewidth = 3.0, c = 'gray')\n",
        "\n",
        "\n",
        "#plt.plot(times_DANA * (float(D)),losses_DANA, label= 'dana, constant', linewidth = 3.0, c = 'orange', alpha = 0.2) #exponent gamma_1 = {:,f}'.format(exponent)+', gamma2={:,}'.format(scaling_gamma_2_momentum)+'/traceK', linewidth = 3.0)\n",
        "#plt.plot(odeTimes_dana * (float(D)), odeRisks_dana, label= 'ode (dana, constant, empirical old)', linewidth = 3.0, c = 'orange')\n",
        "#plt.plot(odeTimes_dana_1 * (float(D)), odeRisks_dana_1, label= 'ode (dana, constant, empirical new)', linewidth = 3.0, c = 'orange', alpha = 0.6)\n",
        "#plt.plot(odeTimes_dana_theory * (float(D)), odeRisks_dana_theory, label= 'ode (dana, deterministic)', linewidth = 3.0, c = 'red')\n",
        "\n",
        "\n",
        "plt.plot(times_DANA_decay * (float(D)),losses_DANA_decay, label= 'dana, decay', linewidth = 3.0, c = 'green', alpha = 0.2) #exponent gamma_1 = {:,f}'.format(exponent)+', gamma2={:,}'.format(scaling_gamma_2_momentum)+'/traceK', linewidth = 3.0)\n",
        "#plt.plot(odeTimes_dana_decay * (float(D)), odeRisks_dana_decay, label= 'ode (dana, decay, empirical, old)', linewidth = 3.0, c = 'green')\n",
        "plt.plot(odeTimes_dana_decay_1 * (float(D)), odeRisks_dana_decay_1, label= 'ode (dana, decay, emp. new)', linewidth = 3.0, c = 'green', alpha = 0.6)\n",
        "#plt.plot(odeTimes_dana_decay_theory * (float(D)), odeRisks_dana_decay_theory, label= 'ode (dana, decay, deterministic)', linewidth = 3.0, c = 'black')\n",
        "\n",
        "\n",
        "plt.plot(odeTimes_sgd_theory * (float(D)), odeRisks_sgd_theory, label= 'ode (sgd, deterministic)', linewidth = 3.0, c = 'gray')\n",
        "plt.plot(odeTimes_dana_decay_theory * (float(D)), odeRisks_dana_decay_theory, label= 'ode (dana, decay, deterministic)', linewidth = 3.0, c = 'black')\n",
        "plt.plot(odeTimes_sgd_theory_approximate * (float(D)), odeRisks_sgd_theory_approximate, label= 'approx. ode (sgd, deterministic)', linewidth = 3.0, c = 'red')\n",
        "plt.plot(odeTimes_dana_decay_theory_approximate * (float(D)), odeRisks_dana_decay_theory_approximate, label= 'approx. ode (dana, decay, deterministic)', linewidth = 3.0, c = 'red', linestyle = 'dashed')\n",
        "\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "\n",
        "leg = plt.legend(loc='upper right', fontsize='10')\n"
      ],
      "metadata": {
        "id": "6XiTj2u68nQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}